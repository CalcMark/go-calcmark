package lexer

import (
	"testing"
)

func TestTokenizeNumbers(t *testing.T) {
	tests := []struct {
		name     string
		input    string
		expected []TokenType
		values   []string
	}{
		{
			name:     "simple number",
			input:    "42",
			expected: []TokenType{NUMBER, EOF},
			values:   []string{"42", ""},
		},
		{
			name:     "decimal number",
			input:    "3.14",
			expected: []TokenType{NUMBER, EOF},
			values:   []string{"3.14", ""},
		},
		{
			name:     "number with commas",
			input:    "1,000",
			expected: []TokenType{NUMBER, EOF},
			values:   []string{"1000", ""},
		},
		{
			name:     "number with underscores",
			input:    "1_000_000",
			expected: []TokenType{NUMBER, EOF},
			values:   []string{"1000000", ""},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			lex := NewLexer(input); tokens, err := lex.Tokenize(tt.input)
			if err != nil {
				t.Fatalf("unexpected error: %v", err)
			}

			if len(tokens) != len(tt.expected) {
				t.Fatalf("expected %d tokens, got %d", len(tt.expected), len(tokens))
			}

			for i, token := range tokens {
				if token.Type != tt.expected[i] {
					t.Errorf("token %d: expected type %v, got %v", i, tt.expected[i], token.Type)
				}
				if token.Value != tt.values[i] {
					t.Errorf("token %d: expected value %q, got %q", i, tt.values[i], token.Value)
				}
			}
		})
	}
}

func TestTokenizeCurrency(t *testing.T) {
	tests := []struct {
		name     string
		input    string
		expected TokenType
		value    string
	}{
		{"simple currency", "$100", QUANTITY, "100:$"},
		{"currency with commas", "$1,000", QUANTITY, "1000:$"},
		{"currency with decimals", "$1,500.50", QUANTITY, "1500.50:$"},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			lex := NewLexer(input); tokens, err := lex.Tokenize(tt.input)
			if err != nil {
				t.Fatalf("unexpected error: %v", err)
			}

			if tokens[0].Type != tt.expected {
				t.Errorf("expected type %v, got %v", tt.expected, tokens[0].Type)
			}
			if tokens[0].Value != tt.value {
				t.Errorf("expected value %q, got %q", tt.value, tokens[0].Value)
			}
		})
	}
}

func TestTokenizeInvalidCurrency(t *testing.T) {
	_, err := Tokenize("$")
	if err == nil {
		t.Error("expected error for invalid currency, got nil")
	}
}

func TestTokenizeIdentifiers(t *testing.T) {
	tests := []struct {
		name     string
		input    string
		expected TokenType
		value    string
	}{
		{"simple identifier", "salary", IDENTIFIER, "salary"},
		{"identifier with underscores", "weeks_in_year", IDENTIFIER, "weeks_in_year"},
		// BREAKING CHANGE: Spaces no longer allowed in identifiers (needed for multi-token functions)
		// {"identifier with spaces", "weeks in year", IDENTIFIER, "weeks in year"},
		{"unicode identifier", "çµ¦æ–™", IDENTIFIER, "çµ¦æ–™"},
		{"identifier with emoji", "ðŸ’°", IDENTIFIER, "ðŸ’°"},
		// ENCODING_SPEC.md: @ is not a valid identifier (not letter, underscore, or emoji)
		// {"special character identifier", "@", IDENTIFIER, "@"},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			lex := NewLexer(input); tokens, err := lex.Tokenize(tt.input)
			if err != nil {
				t.Fatalf("unexpected error: %v", err)
			}

			if tokens[0].Type != tt.expected {
				t.Errorf("expected type %v, got %v", tt.expected, tokens[0].Type)
			}
			if tokens[0].Value != tt.value {
				t.Errorf("expected value %q, got %q", tt.value, tokens[0].Value)
			}
		})
	}
}

func TestTokenizeBooleans(t *testing.T) {
	tests := []struct {
		name  string
		input string
		value string
	}{
		{"true", "true", "true"},
		{"false", "false", "false"},
		{"True uppercase", "True", "true"},
		{"FALSE uppercase", "FALSE", "false"},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			lex := NewLexer(input); tokens, err := lex.Tokenize(tt.input)
			if err != nil {
				t.Fatalf("unexpected error: %v", err)
			}

			if tokens[0].Type != BOOLEAN {
				t.Errorf("expected type BOOLEAN, got %v", tokens[0].Type)
			}
			if tokens[0].Value != tt.value {
				t.Errorf("expected value %q, got %q", tt.value, tokens[0].Value)
			}
		})
	}
}

func TestTokenizeOperators(t *testing.T) {
	tests := []struct {
		name     string
		input    string
		expected TokenType
	}{
		{"plus", "1 + 2", PLUS},
		{"minus", "5 - 3", MINUS},
		{"multiply asterisk", "3 * 3", MULTIPLY},
		{"multiply x", "3 x 3", MULTIPLY},
		{"multiply X", "3 X 3", MULTIPLY},
		{"multiply unicode", "3 Ã— 3", MULTIPLY},
		{"divide", "10 / 2", DIVIDE},
		{"modulus", "10 % 3", MODULUS},
		{"exponent **", "2 ** 3", EXPONENT},
		{"exponent ^", "2 ^ 3", EXPONENT},
		{"assign", "x = 5", ASSIGN},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			lex := NewLexer(input); tokens, err := lex.Tokenize(tt.input)
			if err != nil {
				t.Fatalf("unexpected error: %v", err)
			}

			// Operator is the second token (after first operand)
			if len(tokens) < 2 {
				t.Fatalf("expected at least 2 tokens, got %d", len(tokens))
			}

			if tokens[1].Type != tt.expected {
				t.Errorf("expected type %v, got %v", tt.expected, tokens[1].Type)
			}
		})
	}
}

func TestTokenizeComparisons(t *testing.T) {
	tests := []struct {
		name     string
		input    string
		expected TokenType
	}{
		{"greater than", "5 > 3", GREATER_THAN},
		{"less than", "3 < 5", LESS_THAN},
		{"greater equal", "5 >= 3", GREATER_EQUAL},
		{"less equal", "3 <= 5", LESS_EQUAL},
		{"equal", "5 == 5", EQUAL},
		{"not equal", "5 != 3", NOT_EQUAL},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			lex := NewLexer(input); tokens, err := lex.Tokenize(tt.input)
			if err != nil {
				t.Fatalf("unexpected error: %v", err)
			}

			// Operator is the second token
			if tokens[1].Type != tt.expected {
				t.Errorf("expected type %v, got %v", tt.expected, tokens[1].Type)
			}
		})
	}
}

func TestTokenizeExpressions(t *testing.T) {
	tests := []struct {
		name        string
		input       string
		tokenTypes  []TokenType
		tokenValues []string
	}{
		{
			name:        "simple multiplication",
			input:       "3 * 3",
			tokenTypes:  []TokenType{NUMBER, MULTIPLY, NUMBER, EOF},
			tokenValues: []string{"3", "*", "3", ""},
		},
		{
			name:        "assignment",
			input:       "salary = 52000",
			tokenTypes:  []TokenType{IDENTIFIER, ASSIGN, NUMBER, EOF},
			tokenValues: []string{"salary", "=", "52000", ""},
		},
		{
			name:        "currency multiplication",
			input:       "$1,000 * 52",
			tokenTypes:  []TokenType{QUANTITY, MULTIPLY, NUMBER, EOF},
			tokenValues: []string{"1000:$", "*", "52", ""},
		},
		// BREAKING CHANGE: Spaces no longer allowed in identifiers
		// {
		// 	name:        "identifier with spaces",
		// 	input:       "weeks in year = 52",
		// 	tokenTypes:  []TokenType{IDENTIFIER, ASSIGN, NUMBER, EOF},
		// 	tokenValues: []string{"weeks in year", "=", "52", ""},
		// },
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			lex := NewLexer(input); tokens, err := lex.Tokenize(tt.input)
			if err != nil {
				t.Fatalf("unexpected error: %v", err)
			}

			if len(tokens) != len(tt.tokenTypes) {
				t.Fatalf("expected %d tokens, got %d", len(tt.tokenTypes), len(tokens))
			}

			for i, token := range tokens {
				if token.Type != tt.tokenTypes[i] {
					t.Errorf("token %d: expected type %v, got %v", i, tt.tokenTypes[i], token.Type)
				}
				if token.Value != tt.tokenValues[i] {
					t.Errorf("token %d: expected value %q, got %q", i, tt.tokenValues[i], token.Value)
				}
			}
		})
	}
}

func TestTokenizeMultiline(t *testing.T) {
	lex := NewLexer(input); tokens, err := lex.Tokenize("x = 5\ny = 10")
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}

	// Count newlines
	newlineCount := 0
	for _, token := range tokens {
		if token.Type == NEWLINE {
			newlineCount++
		}
	}

	if newlineCount != 1 {
		t.Errorf("expected 1 newline, got %d", newlineCount)
	}
}

func TestTokenizePositions(t *testing.T) {
	lex := NewLexer(input); tokens, err := lex.Tokenize("x = 5")
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}

	// Check line and column positions
	expectedPositions := []struct {
		line   int
		column int
	}{
		{1, 1}, // x
		{1, 3}, // =
		{1, 5}, // 5
		{1, 6}, // EOF
	}

	for i, token := range tokens {
		if token.Line != expectedPositions[i].line {
			t.Errorf("token %d: expected line %d, got %d", i, expectedPositions[i].line, token.Line)
		}
		if token.Column != expectedPositions[i].column {
			t.Errorf("token %d: expected column %d, got %d", i, expectedPositions[i].column, token.Column)
		}
	}
}

func TestTokenizeXAsIdentifier(t *testing.T) {
	// 'x' should be an identifier when not following a number
	lex := NewLexer(input); tokens, err := lex.Tokenize("x + 5")
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}

	if tokens[0].Type != IDENTIFIER {
		t.Errorf("expected IDENTIFIER, got %v", tokens[0].Type)
	}
	if tokens[0].Value != "x" {
		t.Errorf("expected value %q, got %q", "x", tokens[0].Value)
	}
}

func TestTokenizeXAsMultiply(t *testing.T) {
	// 'x' should be multiply when following a number
	lex := NewLexer(input); tokens, err := lex.Tokenize("3 x 4")
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}

	if tokens[1].Type != MULTIPLY {
		t.Errorf("expected MULTIPLY, got %v", tokens[1].Type)
	}
}

func TestTokenizeComplexExpression(t *testing.T) {
	// BREAKING CHANGE: Use underscores instead of spaces in identifiers
	input := "total_income = $5,000 + bonus"
	lex := NewLexer(input); tokens, err := lex.Tokenize(input)
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}

	expectedTypes := []TokenType{
		IDENTIFIER, // "total_income"
		ASSIGN,     // "="
		QUANTITY,   // "$5,000"
		PLUS,       // "+"
		IDENTIFIER, // "bonus"
		EOF,
	}

	if len(tokens) != len(expectedTypes) {
		t.Fatalf("expected %d tokens, got %d", len(expectedTypes), len(tokens))
	}

	for i, token := range tokens {
		if token.Type != expectedTypes[i] {
			t.Errorf("token %d: expected type %v, got %v", i, expectedTypes[i], token.Type)
		}
	}
}

func TestTokenizeUnexpectedCharacter(t *testing.T) {
	// Test that ! alone (not !=) causes an error
	_, err := Tokenize("5 ! 3")
	if err == nil {
		t.Error("expected error for unexpected character '!', got nil")
	}
}

func TestTokenizeEmptyString(t *testing.T) {
	lex := NewLexer(input); tokens, err := lex.Tokenize("")
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}

	if len(tokens) != 1 {
		t.Fatalf("expected 1 token (EOF), got %d", len(tokens))
	}

	if tokens[0].Type != EOF {
		t.Errorf("expected EOF, got %v", tokens[0].Type)
	}
}

func TestTokenizeWhitespaceOnly(t *testing.T) {
	lex := NewLexer(input); tokens, err := lex.Tokenize("   \t  ")
	if err != nil {
		t.Fatalf("unexpected error: %v", err)
	}

	if len(tokens) != 1 || tokens[0].Type != EOF {
		t.Error("expected only EOF token for whitespace-only input")
	}
}
