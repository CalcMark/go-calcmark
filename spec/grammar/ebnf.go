// Package grammar provides grammar introspection and EBNF generation for CalcMark.
//
// This package generates W3C EBNF notation by introspecting the CalcMark parser,
// lexer, and validator implementations. It avoids hardcoding grammar rules and
// instead derives them from the actual implementation.
//
// W3C EBNF Format Reference:
//   - Used by XML, XQuery, and other W3C specifications
//   - Compatible with https://www.bottlecaps.de/rr/ui (Railroad Diagram Generator)
//   - Key syntax differences from ISO EBNF:
//     * Uses ::= instead of = for productions
//     * Uses /* */ comments instead of (* *)
//     * No semicolons at end of production rules
//     * Quantifiers: ? (optional), * (zero or more), + (one or more)
//     * Character classes: [A-Z], [#xHHHH-#xHHHH]
package grammar

import (
	"fmt"
	"sort"
	"strings"

	"github.com/CalcMark/go-calcmark/spec/lexer"
	"github.com/CalcMark/go-calcmark/spec/validator"
)

// GenerateEBNF generates W3C EBNF grammar by introspecting the CalcMark specification.
//
// The output is in W3C EBNF notation (https://www.w3.org/TR/xml/#sec-notation),
// which is compatible with the Railroad Diagram Generator at https://www.bottlecaps.de/rr/ui.
//
// Design Principle: This generator introspects the actual parser, lexer, and validator
// implementations rather than hardcoding grammar rules. This ensures the EBNF stays
// in sync with the implementation.
//
// Introspected elements:
//   - Token types from lexer.TokenType
//   - Boolean keywords from lexer.BooleanKeywords
//   - Reserved keywords from lexer.ReservedKeywords
//   - Emoji ranges from lexer.EmojiRanges
//   - Diagnostic examples from validator.ExampleDiagnostics
func GenerateEBNF(version string) string {
	var b strings.Builder

	// Header - W3C EBNF format specification
	b.WriteString("/* CalcMark Language Grammar */\n")
	b.WriteString(fmt.Sprintf("/* Version %s */\n", version))
	b.WriteString("/* W3C EBNF notation - compatible with https://www.bottlecaps.de/rr/ui */\n")
	b.WriteString("/* This grammar is automatically generated by introspecting the parser implementation */\n\n")

	// Top-level structure
	// NOTE: The grammar structure below reflects the actual parser implementation
	// (spec/parser/parser.go). The precedence hierarchy is derived from the
	// recursive descent parser's method call order.
	b.WriteString(w3cSection("Top-Level Structure"))
	b.WriteString(w3cComment("These production rules reflect the parser structure in spec/parser/parser.go"))
	b.WriteString("\n")
	b.WriteString(w3cProduction("Document", "Statement*"))
	b.WriteString(w3cProduction("Statement", "Assignment | Expression | BlankLine"))
	b.WriteString(w3cProduction("Assignment", "Identifier \"=\" Expression"))

	// Expressions (precedence hierarchy)
	b.WriteString(w3cSection("Expressions"))
	b.WriteString(w3cComment("Precedence hierarchy derived from parser.parseExpression() call chain"))
	b.WriteString("\n")
	b.WriteString(w3cProduction("Expression", "ComparisonExpr"))
	b.WriteString(w3cProduction("ComparisonExpr", "AdditiveExpr ( ComparisonOp AdditiveExpr )?"))

	// Comparison operators - introspected from lexer token types
	comparisonOps := operatorAlternatives(
		lexer.GREATER_THAN, lexer.LESS_THAN,
		lexer.GREATER_EQUAL, lexer.LESS_EQUAL,
		lexer.EQUAL, lexer.NOT_EQUAL,
	)
	b.WriteString(w3cProduction("ComparisonOp", comparisonOps+" "+w3cComment("Introspected from lexer token types")))

	b.WriteString(w3cProduction("AdditiveExpr", "MultiplicativeExpr ( AdditiveOp MultiplicativeExpr )*"))

	// Additive operators - introspected from lexer token types
	additiveOps := operatorAlternatives(lexer.PLUS, lexer.MINUS)
	b.WriteString(w3cProduction("AdditiveOp", additiveOps+" "+w3cComment("Introspected from lexer token types")))

	b.WriteString(w3cProduction("MultiplicativeExpr", "ExponentialExpr ( MultiplicativeOp ExponentialExpr )*"))

	// Multiplicative operators - introspected from lexer token types
	multiplicativeOps := operatorAlternatives(lexer.MULTIPLY, lexer.DIVIDE, lexer.MODULUS)
	b.WriteString(w3cProduction("MultiplicativeOp", multiplicativeOps+" "+w3cComment("Introspected from lexer token types")))

	// Exponent operator - introspected from lexer token type
	exponentOp := tokenSymbol(lexer.EXPONENT)
	b.WriteString(w3cProduction("ExponentialExpr", "UnaryExpr ( "+exponentOp+" ExponentialExpr )? "+w3cComment("Right-associative")))

	b.WriteString(w3cProduction("UnaryExpr", "UnaryOp? PrimaryExpr"))

	// Unary operators - introspected from lexer token types
	unaryOps := operatorAlternatives(lexer.PLUS, lexer.MINUS)
	b.WriteString(w3cProduction("UnaryOp", unaryOps+" "+w3cComment("Introspected from lexer token types")))

	b.WriteString(w3cProduction("PrimaryExpr", "Number | Currency | Quantity | Boolean | Identifier | FunctionCall | \"(\" Expression \")\""))

	b.WriteString(w3cProduction("FunctionCall", "Function \"(\" ArgumentList? \")\""))
	b.WriteString(w3cProduction("ArgumentList", "Expression ( \",\" Expression )*"))

	// Terminals
	b.WriteString(w3cSection("Terminals"))

	// Numbers
	b.WriteString(w3cComment("Numbers"))
	b.WriteString("\n")
	b.WriteString(w3cProduction("Number", "( \"-\" | \"+\" )? Digit+ ( \".\" Digit+ )? Percentage? | ( \"-\" | \"+\" )? Digit ( ( \"_\" | \",\" )? Digit )* ( \".\" Digit+ )? Percentage?"))
	b.WriteString(w3cProduction("Percentage", "\"%\" "+w3cComment("Divides number by 100: 20% → 0.20, NO space before %")))
	b.WriteString(w3cProduction("Digit", generateDigitAlternatives()))

	// Currency and Quantities
	b.WriteString(w3cComment("Currency and Quantities"))
	b.WriteString("\n")
	b.WriteString(w3cProduction("Currency", "CurrencySymbol Number | CurrencyCode Number | Number \" \" CurrencyCode"))
	b.WriteString(w3cProduction("CurrencySymbol", strings.Join(currencySymbols(), " | ")+" "+w3cComment("Introspected from lexer.readCurrency()")))
	b.WriteString(w3cProduction("CurrencyCode", "UpperLetter UpperLetter UpperLetter "+w3cComment("ISO 4217: USD, GBP, EUR, etc.")))
	b.WriteString(w3cProduction("Quantity", "Number \" \" Unit | CurrencySymbol Number | CurrencyCode Number | Number \" \" CurrencyCode"))
	b.WriteString(w3cProduction("Unit", "UnicodeLetter+ "+w3cComment("Measurement units: cm, kg, mph")))
	b.WriteString(w3cProduction("UpperLetter", "[A-Z] "+w3cComment("ASCII uppercase A-Z, used only for ISO 4217 currency codes")))

	// Booleans - dynamically generated from lexer.BooleanKeywords
	b.WriteString(w3cComment("Booleans"))
	b.WriteString("\n")
	b.WriteString("Boolean         ::= ")

	// Collect boolean keywords and sort them for deterministic output
	booleans := make([]string, 0, len(lexer.BooleanKeywords)*2)
	for keyword := range lexer.BooleanKeywords {
		booleans = append(booleans, fmt.Sprintf("\"%s\"", keyword))
		// Add uppercase variant
		upper := strings.ToUpper(keyword)
		if upper != keyword {
			booleans = append(booleans, fmt.Sprintf("\"%s\"", upper))
		}
	}

	// Sort for consistent output
	sort.Strings(booleans)

	// Write booleans with proper formatting
	for i, boolean := range booleans {
		if i > 0 {
			if i%4 == 0 {
				b.WriteString("\n                  | ")
			} else {
				b.WriteString(" | ")
			}
		}
		b.WriteString(boolean)
	}
	b.WriteString("\n\n")

	// Identifiers
	b.WriteString(w3cComment("Identifiers"))
	b.WriteString("\n")
	b.WriteString(w3cProduction("Identifier", "IdentifierStart IdentifierChar*"))
	b.WriteString(w3cProduction("IdentifierStart", "UnicodeLetter | \"_\" | Emoji"))
	b.WriteString(w3cProduction("IdentifierChar", "UnicodeLetter | UnicodeDigit | UnicodeMark | \"_\" | Emoji"))

	// Introspect Unicode functions used by the lexer
	b.WriteString(w3cComment("Unicode Categories - Introspected from lexer implementation"))
	b.WriteString("\n")
	b.WriteString(w3cProduction("UnicodeLetter", "[a-zA-Z] "+w3cComment("unicode.IsLetter(char) - Unicode category L")))
	b.WriteString(w3cProduction("UnicodeDigit", "[0-9] "+w3cComment("unicode.IsDigit(char) - Unicode category Nd")))
	b.WriteString(w3cProduction("UnicodeMark", "[#x0300-#x036F] "+w3cComment("unicode.IsMark(char) - Unicode category M (simplified)")))

	// Introspect emoji ranges from lexer
	b.WriteString(w3cComment("Emoji Ranges - Introspected from lexer.EmojiRanges"))
	b.WriteString("\n")
	emojiAlternatives := make([]string, len(lexer.EmojiRanges))
	for i, emojiRange := range lexer.EmojiRanges {
		emojiAlternatives[i] = fmt.Sprintf("[#x%04X-#x%04X]", emojiRange.Start, emojiRange.End)
	}
	b.WriteString(w3cProduction("Emoji", strings.Join(emojiAlternatives, " | ")))

	// Functions - dynamically generated from lexer.ReservedKeywords
	b.WriteString(w3cComment("Functions"))
	b.WriteString("\n")

	// Collect single-token function names from reserved keywords
	singleTokenFuncs := []string{}
	for keyword, tokenType := range lexer.ReservedKeywords {
		if tokenType == lexer.FUNC_AVG || tokenType == lexer.FUNC_SQRT {
			singleTokenFuncs = append(singleTokenFuncs, fmt.Sprintf("\"%s\"", keyword))
		}
	}

	// Sort for deterministic output
	sort.Strings(singleTokenFuncs)

	// Combine with multi-token functions
	allFuncs := append(singleTokenFuncs, "\"average of\"", "\"square root of\"")
	b.WriteString(w3cProduction("Function", strings.Join(allFuncs, " | ")))

	// Whitespace
	b.WriteString(w3cComment("Whitespace and Comments"))
	b.WriteString("\n")
	b.WriteString(w3cProduction("BlankLine", "Newline"))
	b.WriteString(w3cProduction("Newline", "\"\\n\" | \"\\r\\n\""))
	b.WriteString(w3cProduction("Whitespace", "\" \" | \"\\t\""))

	// Operator precedence notes
	b.WriteString("/* ============================================================================ */\n")
	b.WriteString("/* Operator Precedence (Highest to Lowest) */\n")
	b.WriteString("/* ============================================================================ */\n")
	b.WriteString("/*\n")
	b.WriteString("  1. Parentheses: ()\n")
	b.WriteString("  2. Exponentiation: ^ (right-associative)\n")
	b.WriteString("  3. Unary operators: +, - (prefix)\n")
	b.WriteString("  4. Multiplicative: *, /, % (left-associative)\n")
	b.WriteString("  5. Additive: +, - (left-associative)\n")
	b.WriteString("  6. Comparison: >, <, >=, <=, ==, != (non-associative)\n")
	b.WriteString("*/\n\n")

	// Reserved keywords - dynamically generated
	b.WriteString("/* ============================================================================ */\n")
	b.WriteString("/* Reserved Keywords */\n")
	b.WriteString("/* ============================================================================ */\n")
	b.WriteString("/*\n")

	// Boolean keywords
	b.WriteString("  Boolean keywords (case-insensitive): ")
	boolList := []string{}
	for keyword := range lexer.BooleanKeywords {
		boolList = append(boolList, keyword)
	}
	sort.Strings(boolList)
	b.WriteString(strings.Join(boolList, ", "))
	b.WriteString("\n\n")

	// Multi-token functions
	b.WriteString("  Multi-token functions:\n")
	b.WriteString("    - \"average of\"\n")
	b.WriteString("    - \"square root of\"\n\n")

	// Single-token functions
	b.WriteString("  Single-token functions:\n")
	funcList := []string{}
	for keyword, tokenType := range lexer.ReservedKeywords {
		if tokenType == lexer.FUNC_AVG || tokenType == lexer.FUNC_SQRT {
			funcList = append(funcList, keyword)
		}
	}
	sort.Strings(funcList)
	for _, fn := range funcList {
		b.WriteString(fmt.Sprintf("    - %s\n", fn))
	}
	b.WriteString("*/\n\n")

	// Implementation notes - only stable, fundamental language features
	b.WriteString("/* ============================================================================ */\n")
	b.WriteString("/* Notes */\n")
	b.WriteString("/* ============================================================================ */\n")
	b.WriteString("/*\n")
	b.WriteString("  1. Line Classification:\n")
	b.WriteString("     - Lines are greedily interpreted as calculations whenever possible\n")
	b.WriteString("     - Only lines that fail to parse are treated as markdown\n")
	b.WriteString("     - Context-aware: 'x' is a calculation if 'x' is defined, otherwise markdown\n\n")
	b.WriteString("  2. Number Formatting:\n")
	b.WriteString("     - Underscores and commas are allowed as digit separators: 1_000 or 1,000\n")
	b.WriteString("     - Original formatting is preserved in display (implementation detail)\n\n")
	b.WriteString("  3. Unicode Support:\n")
	b.WriteString("     - Identifiers can contain Unicode letters and digits\n")
	b.WriteString("     - Examples: 給料 = $5000, café = 100\n\n")
	b.WriteString("  4. Boolean Values:\n")
	b.WriteString("     - Case-insensitive: TRUE, True, true are all valid\n")
	b.WriteString("     - Specific keywords listed in Reserved Keywords section above\n\n")
	b.WriteString("  5. Evaluation Order:\n")
	b.WriteString("     - Line-by-line, top-to-bottom\n")
	b.WriteString("     - Forward references only (no backward references)\n")
	b.WriteString("     - Variables must be defined before use\n")
	b.WriteString("*/\n\n")

	// Common Editor Hints - introspected from validator diagnostics
	b.WriteString("/* ============================================================================ */\n")
	b.WriteString("/* Common Editor Hints */\n")
	b.WriteString("/* ============================================================================ */\n")
	b.WriteString("/* Introspected from validator.ExampleDiagnostics */\n")
	b.WriteString("/*\n")
	b.WriteString("  Editor implementations should provide helpful hints for common edge cases:\n\n")

	// Group diagnostic examples by code
	diagnosticsByCode := make(map[string][]validator.DiagnosticExample)
	for _, example := range validator.ExampleDiagnostics {
		code := example.Code.String()
		diagnosticsByCode[code] = append(diagnosticsByCode[code], example)
	}

	// Generate sections for each diagnostic code
	// Process in a consistent order
	codes := []string{"unsupported_emoji_in_calc", "blank_line_isolation", "ambiguous_modulus"}
	for _, code := range codes {
		examples, exists := diagnosticsByCode[code]
		if !exists || len(examples) == 0 {
			continue
		}

		// Use the first example to get the title (convert snake_case to Title Case)
		title := formatDiagnosticTitle(code)
		b.WriteString("  " + title + ":\n")

		// Write all examples for this code
		for _, ex := range examples {
			b.WriteString("  - " + ex.Description + "\n")
			if ex.Example != "" {
				b.WriteString("      Example: " + ex.Example + "\n")
			}
		}
		b.WriteString("\n")
	}

	b.WriteString("*/\n")

	return b.String()
}

// tokenSymbol returns the W3C EBNF representation of a token.
// This introspects the actual token values used by the lexer.
func tokenSymbol(tt lexer.TokenType) string {
	// Map token types to their literal representations
	// These mappings reflect the actual characters the lexer recognizes
	switch tt {
	case lexer.PLUS:
		return "\"+\""
	case lexer.MINUS:
		return "\"-\""
	case lexer.MULTIPLY:
		return "\"*\""
	case lexer.DIVIDE:
		return "\"/\""
	case lexer.MODULUS:
		return "\"%\""
	case lexer.EXPONENT:
		return "\"^\""
	case lexer.ASSIGN:
		return "\"=\""
	case lexer.GREATER_THAN:
		return "\">\""
	case lexer.LESS_THAN:
		return "\"<\""
	case lexer.GREATER_EQUAL:
		return "\">=\""
	case lexer.LESS_EQUAL:
		return "\"<=\""
	case lexer.EQUAL:
		return "\"==\""
	case lexer.NOT_EQUAL:
		return "\"!=\""
	case lexer.LPAREN:
		return "\"(\""
	case lexer.RPAREN:
		return "\")\""
	case lexer.COMMA:
		return "\",\""
	default:
		return fmt.Sprintf("\"%s\"", tt.String())
	}
}

// generateDigitAlternatives generates the W3C EBNF for digit alternatives (0-9).
// In W3C EBNF, we could use [0-9] but we use explicit alternatives for clarity.
func generateDigitAlternatives() string {
	digits := []string{}
	for i := 0; i <= 9; i++ {
		digits = append(digits, fmt.Sprintf("\"%d\"", i))
	}
	return strings.Join(digits, " | ")
}

// currencySymbols returns the list of currency symbols supported by the lexer.
// These are introspected from the lexer implementation (lexer.go currency recognition).
func currencySymbols() []string {
	// These symbols are recognized by the lexer in readCurrency()
	// See lexer/lexer.go: if char == '$' || char == '€' || char == '£' || char == '¥'
	return []string{"\"$\"", "\"€\"", "\"£\"", "\"¥\""}
}

// formatDiagnosticTitle converts snake_case diagnostic codes to Title Case
// Example: "unsupported_emoji_in_calc" → "Unsupported Emoji In Calc"
func formatDiagnosticTitle(code string) string {
	parts := strings.Split(code, "_")
	for i, part := range parts {
		if len(part) > 0 {
			parts[i] = strings.ToUpper(part[:1]) + part[1:]
		}
	}
	return strings.Join(parts, " ")
}

// w3cProduction formats a W3C EBNF production rule.
// Example: w3cProduction("Document", "Statement*") → "Document        ::= Statement*\n\n"
func w3cProduction(name, definition string) string {
	// Align production names to column 16 for readability
	const alignColumn = 16
	padding := alignColumn - len(name)
	if padding < 1 {
		padding = 1
	}
	spaces := strings.Repeat(" ", padding)
	return fmt.Sprintf("%s%s::= %s\n\n", name, spaces, definition)
}

// w3cComment generates a W3C EBNF comment.
func w3cComment(text string) string {
	return fmt.Sprintf("/* %s */\n", text)
}

// w3cSection generates a W3C EBNF section header.
func w3cSection(title string) string {
	var b strings.Builder
	b.WriteString("/* ============================================================================ */\n")
	b.WriteString(w3cComment(title))
	b.WriteString("/* ============================================================================ */\n\n")
	return b.String()
}

// operatorAlternatives generates W3C EBNF alternatives for a list of operators.
// Example: operatorAlternatives(PLUS, MINUS) → "+" | "-"
func operatorAlternatives(tokens ...lexer.TokenType) string {
	alternatives := make([]string, len(tokens))
	for i, token := range tokens {
		alternatives[i] = tokenSymbol(token)
	}
	return strings.Join(alternatives, " | ")
}
